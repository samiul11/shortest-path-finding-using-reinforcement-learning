{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "#configuration parameters\n",
    "#startLoc should be changed from here\n",
    "config = {\n",
    "    \"height\": 30,\n",
    "    \"width\": 30,\n",
    "    \"startLoc\": [2, 4],\n",
    "    \"endLoc\": [25, 18],\n",
    "    #if you want to add new obstacles,then just add another topLeft and bottomRight corner dictionary to list\n",
    "    #weight for the semi soft obstacle\n",
    "    \"ultraSoftWeight\": -2,\n",
    "    \"ultraSoftObstacles\": [\n",
    "        {\"topLeft\": [7, 5], \"bottomRight\": [12, 10]},\n",
    "        {\"topLeft\": [13, 6], \"bottomRight\": [14, 9]},\n",
    "        {\"topLeft\": [15, 9], \"bottomRight\": [16, 11]},\n",
    "        {\"topLeft\": [8, 11], \"bottomRight\": [10, 12]},\n",
    "        {\"topLeft\": [17, 9], \"bottomRight\": [18, 17]},\n",
    "        {\"topLeft\": [10, 13], \"bottomRight\": [12, 14]},\n",
    "        {\"topLeft\": [12, 14], \"bottomRight\": [16, 17]},\n",
    "    ],\n",
    "    #weight for the semi soft obstacle\n",
    "    \"semiSoftWeight\": -3,\n",
    "    \"semiSoftObstacles\": [\n",
    "        {\"topLeft\": [8, 6], \"bottomRight\": [11, 9]},\n",
    "        {\"topLeft\": [12, 7], \"bottomRight\": [13, 10]},\n",
    "        {\"topLeft\": [9, 10], \"bottomRight\": [10, 11]},\n",
    "        {\"topLeft\": [14, 10], \"bottomRight\": [15, 11]},\n",
    "        {\"topLeft\": [16, 12], \"bottomRight\": [17, 16]},\n",
    "        {\"topLeft\": [11, 12], \"bottomRight\": [12, 13]},\n",
    "        {\"topLeft\": [13, 14], \"bottomRight\": [15, 16]},\n",
    "        \n",
    "        {\"topLeft\": [11, 25], \"bottomRight\": [15, 27]},\n",
    "        {\"topLeft\": [12, 23], \"bottomRight\": [19, 25]},\n",
    "        {\"topLeft\": [13, 21], \"bottomRight\": [24, 24]},\n",
    "        {\"topLeft\": [18, 20], \"bottomRight\": [24, 20]},\n",
    "        {\"topLeft\": [20, 18], \"bottomRight\": [24, 19]},\n",
    "    ],\n",
    "    \"softObstacles\": [\n",
    "        {\"topLeft\": [9, 7], \"bottomRight\": [10, 9]},\n",
    "        {\"topLeft\": [11, 8], \"bottomRight\": [12, 8]}, \n",
    "        {\"topLeft\": [12, 9], \"bottomRight\": [12,10]},\n",
    "        {\"topLeft\": [10, 10], \"bottomRight\": [10, 10]},\n",
    "        {\"topLeft\": [11, 11], \"bottomRight\": [14, 11]},\n",
    "        {\"topLeft\": [12, 12], \"bottomRight\": [15, 12]},\n",
    "        {\"topLeft\": [13, 13], \"bottomRight\": [16, 13]},\n",
    "        {\"topLeft\": [14, 14], \"bottomRight\": [16, 15]},\n",
    "        \n",
    "        {\"topLeft\": [12, 25], \"bottomRight\": [14, 26]},\n",
    "        {\"topLeft\": [13, 23], \"bottomRight\": [14, 24]},\n",
    "        {\"topLeft\": [14, 22], \"bottomRight\": [18, 24]},\n",
    "        {\"topLeft\": [18, 21], \"bottomRight\": [23, 23]},\n",
    "        {\"topLeft\": [21, 19], \"bottomRight\": [23, 20]},\n",
    "    ],\n",
    "    #weight for the soft obstacle\n",
    "    \"softWeight\": -4,\n",
    "    \"hardObstacles\": [\n",
    "        {\"topLeft\": [9, 8], \"bottomRight\": [10, 8]},\n",
    "        \n",
    "        {\"topLeft\": [11, 9], \"bottomRight\": [11, 10]},\n",
    "        {\"topLeft\": [12, 11], \"bottomRight\": [12, 11]},\n",
    "        \n",
    "        {\"topLeft\": [13, 12], \"bottomRight\": [14, 12]},\n",
    "        \n",
    "        {\"topLeft\": [15, 13], \"bottomRight\": [15, 14]},\n",
    "        {\"topLeft\": [13, 25], \"bottomRight\": [13, 25]}, \n",
    "        \n",
    "        {\"topLeft\": [14, 23], \"bottomRight\": [14, 24]},\n",
    "        {\"topLeft\": [14, 23], \"bottomRight\": [17, 23]},\n",
    "        \n",
    "        {\"topLeft\": [18, 22], \"bottomRight\": [18, 23]},\n",
    "        {\"topLeft\": [19, 22], \"bottomRight\": [22, 22]},\n",
    "        \n",
    "        {\"topLeft\": [22, 20], \"bottomRight\": [22, 22]},\n",
    "#         {\"topLeft\": [15, 24], \"bottomRight\": [14, 25]}\n",
    "        \n",
    "    ],\n",
    "    #weight for the hard obstacle\n",
    "    \"hardWeight\": -1000,\n",
    "    \"reward\": 1000000       \n",
    "}\n",
    "#class that finds the optimal path\n",
    "class PathFinder:\n",
    "    #initializing the environment\n",
    "    def __init__(self, config):        \n",
    "        self.environment_rows = config['height']\n",
    "        self.environment_columns = config['width'] \n",
    "        #initializing qValues\n",
    "        #our goal is to learn this values from many experimental runs\n",
    "        self.q_values = np.zeros((self.environment_rows, self.environment_columns, 4))\n",
    "        #set of possible actions\n",
    "        self.actions = ['up', 'right', 'down', 'left']\n",
    "        #rewards matrix(set to -1 at start)\n",
    "        self.rewards = np.full((self.environment_rows, self.environment_columns), -1)\n",
    "        #initializing ultraSoftObstacle with ultraSoftWeights\n",
    "        for i in config[\"ultraSoftObstacles\"]:\n",
    "            self.rewards[i[\"topLeft\"][0]:i[\"bottomRight\"][0]+1, i[\"topLeft\"][1]:i[\"bottomRight\"][1]+1] = config[\"ultraSoftWeight\"]\n",
    "        #initializing semiSoftObstacle with semiSoftWeights\n",
    "        for i in config[\"semiSoftObstacles\"]:\n",
    "            self.rewards[i[\"topLeft\"][0]:i[\"bottomRight\"][0]+1, i[\"topLeft\"][1]:i[\"bottomRight\"][1]+1] = config[\"semiSoftWeight\"]\n",
    "        #initializing softObstacle with softWeights\n",
    "        for i in config[\"softObstacles\"]:\n",
    "            self.rewards[i[\"topLeft\"][0]:i[\"bottomRight\"][0]+1, i[\"topLeft\"][1]:i[\"bottomRight\"][1]+1] = config[\"softWeight\"]\n",
    "        #initializing hardObstacles with hardWeights\n",
    "        for i in config[\"hardObstacles\"]:\n",
    "            self.rewards[i[\"topLeft\"][0]:i[\"bottomRight\"][0]+1, i[\"topLeft\"][1]:i[\"bottomRight\"][1]+1] = config[\"hardWeight\"]  \n",
    "        #will raise an error if startpoint is on hard obstacle\n",
    "        assert self.rewards[config[\"startLoc\"][0],config[\"startLoc\"][1]] != config[\"hardWeight\"], \"Start point can not be inside hard obstacles!\"\n",
    "        #initializing startLocation and reward for getting to it\n",
    "        self.rewards[config[\"startLoc\"][0],config[\"startLoc\"][1]] = config['reward']\n",
    "        #print(np.array(self.rewards, dtype=int))\n",
    "    #checking if experiment must stop\n",
    "    #stop conditions 1.crushed into hard obstacle\n",
    "                   # 2.reached startLoc\n",
    "    def is_terminal_state(self, current_row_index, current_column_index):\n",
    "        return (self.rewards[current_row_index, current_column_index] == config[\"hardWeight\"]) or ((self.rewards[current_row_index, current_column_index] == config['reward']))\n",
    "    #agent plays many games starting from random position,trying to maximize his reward(reach startLoc)\n",
    "    #this function initializes the agent in valid place\n",
    "    def get_starting_location(self):\n",
    "        current_row_index = np.random.randint(self.environment_rows)\n",
    "        current_column_index = np.random.randint(self.environment_columns)\n",
    "        while self.is_terminal_state(current_row_index, current_column_index):\n",
    "            current_row_index = np.random.randint(self.environment_rows)\n",
    "            current_column_index = np.random.randint(self.environment_columns)\n",
    "        return current_row_index, current_column_index\n",
    "    #function that chooses next action from given [row,column] state\n",
    "    #with probability 100epsilon it chooses best action\n",
    "    #so epsilon is for adding some randomness to our agent\n",
    "    def get_next_action(self, current_row_index, current_column_index, epsilon):\n",
    "        if np.random.random() < epsilon:\n",
    "            return np.argmax(self.q_values[current_row_index, current_column_index])\n",
    "        else:\n",
    "            return np.random.randint(4)   \n",
    "    #once the action is chosen,you need to know the location after making that action\n",
    "    def get_next_location(self, current_row_index, current_column_index, action_index):\n",
    "        new_row_index = current_row_index\n",
    "        new_column_index = current_column_index\n",
    "        if self.actions[action_index] == 'up' and current_row_index > 0:\n",
    "            new_row_index -= 1\n",
    "        elif self.actions[action_index] == 'right' and current_column_index < self.environment_columns - 1:\n",
    "            new_column_index += 1\n",
    "        elif self.actions[action_index] == 'down' and current_row_index < self.environment_rows - 1:\n",
    "            new_row_index += 1\n",
    "        elif self.actions[action_index] == 'left' and current_column_index > 0:\n",
    "            new_column_index -= 1\n",
    "        return new_row_index, new_column_index\n",
    "    #AFTER TRAINING\n",
    "    #accepts the endLocation,and returns the shortest path to that location from startLoc\n",
    "    #once you train the agent,you can call this function(without training again),with different endLocation\n",
    "    #if you want to change the startLocation,you must train agent again\n",
    "    def get_shortest_path(self, start_row_index, start_column_index):\n",
    "        shortest_path = []\n",
    "        if self.is_terminal_state(start_row_index, start_column_index):\n",
    "            return shortest_path\n",
    "        else:\n",
    "            current_row_index, current_column_index = start_row_index, start_column_index\n",
    "            shortest_path.append([current_row_index, current_column_index])\n",
    "        while not self.is_terminal_state(current_row_index, current_column_index):\n",
    "            action_index = self.get_next_action(current_row_index, current_column_index, 1.)\n",
    "            #print(action_index)\n",
    "            current_row_index, current_column_index = self.get_next_location(current_row_index, current_column_index, action_index)\n",
    "            shortest_path.append([current_row_index, current_column_index])\n",
    "        return shortest_path\n",
    "\n",
    "    #training function\n",
    "    #parameters-epoch(number of epoch to train our agent)\n",
    "    def train(self, epoch):\n",
    "        #hyperparameters used in Bellman equation for computing Qvalues\n",
    "        epsilon = 0.8\n",
    "        discount_factor = 0.99 \n",
    "        learning_rate = 0.4\n",
    "\n",
    "        for episode in range(epoch):\n",
    "            row_index, column_index = self.get_starting_location()\n",
    "            \n",
    "            while not self.is_terminal_state(row_index, column_index):\n",
    "                #while experimental run is not finished,update Qtable according to Bellman equation\n",
    "                action_index = self.get_next_action(row_index, column_index, epsilon)\n",
    "\n",
    "                old_row_index, old_column_index = row_index, column_index \n",
    "                row_index, column_index = self.get_next_location(row_index, column_index, action_index)\n",
    "\n",
    "                reward = self.rewards[row_index, column_index]\n",
    "                old_q_value = self.q_values[old_row_index, old_column_index, action_index]\n",
    "                temporal_difference = reward + (discount_factor * np.max(self.q_values[row_index, column_index])) - old_q_value\n",
    "\n",
    "                new_q_value = old_q_value + (learning_rate * temporal_difference)\n",
    "                self.q_values[old_row_index, old_column_index, action_index] = new_q_value\n",
    "\n",
    "#function to visualize given path\n",
    "def visualize_(boardSize, path, start, end):\n",
    "    \n",
    "    rows = boardSize[0] * 20\n",
    "    cols = boardSize[1] * 20\n",
    "    img = np.zeros((cols + 1, rows + 1, 3), dtype = \"uint8\") #3channel\n",
    "\n",
    "    stepX = int(rows / 20) + 1\n",
    "#     step=75\n",
    "    stepY = int(cols / 20) + 1\n",
    "    x = np.linspace(start = 0, stop = rows, num = stepX)\n",
    "    y = np.linspace(start = 0, stop = cols, num = stepY)\n",
    "\n",
    "    v_xy = []\n",
    "    h_xy = []\n",
    "    for i in range(stepX):\n",
    "        v_xy.append([int(x[i]), 0, int(x[i]), cols-1])\n",
    "    for i in range(stepY): \n",
    "        h_xy.append([0, int(y[i]), rows-1, int(y[i])])\n",
    "\n",
    "\n",
    "    for i in range(stepX):\n",
    "        [x1, y1, x2, y2] = v_xy[i]\n",
    "        cv2.line(img, (x1,y1), (x2, y2), (255,255,255), 1)\n",
    "        \n",
    "    for i in range(stepY):\n",
    "        [x1_, y1_, x2_, y2_] = h_xy[i]\n",
    "        cv2.line(img, (x1_,y1_), (x2_ + 1, y2_), (255,255,255), 1)\n",
    "\n",
    "    for obs in config['ultraSoftObstacles']:\n",
    "        cv2.rectangle(img,(obs['topLeft'][1]*20, obs['topLeft'][0]*20),((obs['bottomRight'][1]+1)*20, (obs['bottomRight'][0]+1)*20), (100,50,0),2)\n",
    "\n",
    "    for obs in config['semiSoftObstacles']:\n",
    "        cv2.rectangle(img,(obs['topLeft'][1]*20, obs['topLeft'][0]*20),((obs['bottomRight'][1]+1)*20, (obs['bottomRight'][0]+1)*20), (100,100,0),2)\n",
    "\n",
    "    for obs in config['softObstacles']:\n",
    "        cv2.rectangle(img,(obs['topLeft'][1]*20, obs['topLeft'][0]*20),((obs['bottomRight'][1]+1)*20, (obs['bottomRight'][0]+1)*20), (100,0,100),2)\n",
    "\n",
    "    for obs in config['hardObstacles']:\n",
    "        cv2.rectangle(img,(obs['topLeft'][1]*20, obs['topLeft'][0]*20),((obs['bottomRight'][1]+1)*20, (obs['bottomRight'][0]+1)*20), (255,0,255),2)\n",
    "        \n",
    "    # for i, p in enumerate(path):\n",
    "    #     print(p)\n",
    "    color = (255,0,0)\n",
    "    for ind in range(len(path) - 1):\n",
    "        cv2.line(img, (path[ind][1]*20 + 10,path[ind][0]*20 + 10), (path[ind + 1][1]*20 + 10, path[ind + 1][0]*20 + 10), color, 3)\n",
    "    \n",
    "    # for start in startPoints:\n",
    "    cv2.circle(img,(start[1]*20+10,start[0]*20+10), 3, (0,255,255), -1)\n",
    "    # for end in endPoints:\n",
    "    cv2.circle(img,(end[1]*20+10,end[0]*20+10), 3, (255,0,255), -1)\n",
    "        \n",
    "    cv2.namedWindow('Optimal path', cv2.WINDOW_NORMAL)\n",
    "    cv2.imshow('Optimal path', img)\n",
    "    cv2.waitKey(0)\n",
    "\n",
    "#for dynamic ploting\n",
    "#press any key(e.g. enter) for next step\n",
    "def visualize(path):\n",
    "    for p in range(len(path),-1,-1):\n",
    "        visualize_((config['height'], config['width']), path[p:], config['startLoc'], config['endLoc'])\n",
    "    cv2.destroyAllWindows()\n",
    "                \n",
    "try:\n",
    "    short = PathFinder(config)\n",
    "    #trains for 10000 experimental runs(epochs)\n",
    "    short.train(10000)\n",
    "except AssertionError as e:\n",
    "    print(e)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns shortestPath from startLoc(given in config to endLoc also given in config)\n",
    "#if you want to try for differents startLoc,you need to rerun whole thing again\n",
    "#if you want to test for different endPoint you can run this 3 cells(including uncommented one) separately\n",
    "# config['endLoc'] = [0,9]\n",
    "path = short.get_shortest_path(config['endLoc'][0], config['endLoc'][1])\n",
    "visualize(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "#configuration parameters\n",
    "#startLoc should be changed from here\n",
    "config = {\n",
    "    \"height\": 30,\n",
    "    \"width\": 30,\n",
    "    \"startLoc\": [2, 4],\n",
    "    \"endLoc\": [25, 25],\n",
    "    #if you want to add new obstacles,then just add another topLeft and bottomRight corner dictionary to list\n",
    "    \"softObstacles\": [\n",
    "        {\"topLeft\": [9, 7], \"bottomRight\": [10, 9]},\n",
    "        {\"topLeft\": [11, 8], \"bottomRight\": [12, 8]}, \n",
    "        {\"topLeft\": [12, 9], \"bottomRight\": [12,10]},\n",
    "        {\"topLeft\": [10, 10], \"bottomRight\": [10, 10]},\n",
    "        {\"topLeft\": [11, 11], \"bottomRight\": [14, 11]},\n",
    "        {\"topLeft\": [12, 12], \"bottomRight\": [15, 12]},\n",
    "        {\"topLeft\": [13, 13], \"bottomRight\": [16, 13]},\n",
    "        {\"topLeft\": [14, 14], \"bottomRight\": [16, 15]},\n",
    "        \n",
    "        {\"topLeft\": [12, 25], \"bottomRight\": [14, 26]},\n",
    "        {\"topLeft\": [13, 23], \"bottomRight\": [14, 24]},\n",
    "        {\"topLeft\": [15, 22], \"bottomRight\": [18, 24]},\n",
    "        {\"topLeft\": [18, 21], \"bottomRight\": [23, 23]},\n",
    "        {\"topLeft\": [21, 19], \"bottomRight\": [23, 20]},\n",
    "    ],\n",
    "    #weight for the soft obstacle\n",
    "    \"softWeight\": -2,\n",
    "    \"hardObstacles\": [\n",
    "        {\"topLeft\": [9, 8], \"bottomRight\": [10, 8]},\n",
    "        \n",
    "        {\"topLeft\": [11, 9], \"bottomRight\": [11, 10]},\n",
    "        {\"topLeft\": [12, 11], \"bottomRight\": [12, 11]},\n",
    "        \n",
    "        {\"topLeft\": [13, 12], \"bottomRight\": [14, 12]},\n",
    "        \n",
    "        {\"topLeft\": [15, 13], \"bottomRight\": [15, 14]},\n",
    "        {\"topLeft\": [13, 25], \"bottomRight\": [13, 25]}, \n",
    "        \n",
    "        {\"topLeft\": [14, 23], \"bottomRight\": [14, 24]},\n",
    "        {\"topLeft\": [15, 23], \"bottomRight\": [17, 23]},\n",
    "        \n",
    "        {\"topLeft\": [18, 22], \"bottomRight\": [18, 23]},\n",
    "        {\"topLeft\": [19, 22], \"bottomRight\": [22, 22]},\n",
    "        \n",
    "        {\"topLeft\": [22, 20], \"bottomRight\": [22, 22]},\n",
    "#         {\"topLeft\": [15, 24], \"bottomRight\": [14, 25]}\n",
    "        \n",
    "    ],\n",
    "    #weight for the hard obstacle\n",
    "    \"hardWeight\": -1000,\n",
    "    \"reward\": 1000000       \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "environment_rows = config['height']\n",
    "environment_columns = config['width'] \n",
    "        #initializing qValues\n",
    "        #our goal is to learn this values from many experimental runs\n",
    "q_values = np.zeros((environment_rows, environment_columns, 4))\n",
    "        #set of possible actions\n",
    "actions = ['up', 'right', 'down', 'left']\n",
    "        #rewards matrix(set to -1 at start)\n",
    "rewards = np.full((environment_rows, environment_columns), -1)\n",
    "        #initializing softObstacle with softWeights\n",
    "for i in config[\"softObstacles\"]:\n",
    "    rewards[i[\"topLeft\"][0]:i[\"bottomRight\"][0]+1, i[\"topLeft\"][1]:i[\"bottomRight\"][1]+1] = config[\"softWeight\"]\n",
    "        #initializing hardObstacles with hardWeights\n",
    "for i in config[\"hardObstacles\"]:\n",
    "    rewards[i[\"topLeft\"][0]:i[\"bottomRight\"][0]+1, i[\"topLeft\"][1]:i[\"bottomRight\"][1]+1] = config[\"hardWeight\"]  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
